{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y82SFgXnPWNE"
   },
   "source": [
    "# Training Kandinsky 2.2 with LoRA\n",
    "This notebook can train LoRA for both the prior and decoder, as well as do basic inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bKdkBtCqTwug"
   },
   "outputs": [],
   "source": [
    "# @title Install Requirements\n",
    "%%bash\n",
    "git clone https://github.com/ai-forever/diffusers\n",
    "pip install /content/diffusers\n",
    "pip install transformers\n",
    "pip install accelerate\n",
    "pip install bitsandbytes\n",
    "pip install safetensors\n",
    "apt install aria2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jML9E3jXeaBl"
   },
   "outputs": [],
   "source": [
    "# @title Mount Drive (Optional)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OtRi91qbb-3n"
   },
   "outputs": [],
   "source": [
    "# @title Locating Train Data Directory\n",
    "# @markdown Define the location of your training data. This cell will also create a folder based on your input.\n",
    "# @markdown Regularization images are not presently used.\n",
    "import os\n",
    "from IPython.utils import capture\n",
    "\n",
    "%store -r\n",
    "\n",
    "train_data_dir = \"/content/LoRA/train_data\"  # @param {type:'string'}\n",
    "reg_data_dir = \"/content/LoRA/reg_data\"  # @param {type:'string'}\n",
    "\n",
    "for dir in [train_data_dir, reg_data_dir]:\n",
    "    if dir:\n",
    "        with capture.capture_output() as cap:\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            %store dir\n",
    "            del cap\n",
    "\n",
    "print(f\"Your train data directory : {train_data_dir}\")\n",
    "if reg_data_dir:\n",
    "    print(f\"Your reg data directory : {reg_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Cib-x3EUbV64"
   },
   "outputs": [],
   "source": [
    "# @title Extract Dataset\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# @markdown Use this section if your dataset is in a `zip` file and has been uploaded somewhere. This code cell will download your dataset and automatically extract it to the `train_data_dir` if the `unzip_to` variable is empty.\n",
    "root_dir = \"/content\"\n",
    "zipfile_url = \"\" #@param {type:\"string\"}\n",
    "zipfile_name = \"zipfile.zip\"\n",
    "unzip_to = \"\" #@param {type:\"string\"}\n",
    "\n",
    "hf_token = \"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\" #@param {type:\"string\"}\n",
    "user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
    "\n",
    "if unzip_to:\n",
    "    os.makedirs(unzip_to, exist_ok=True)\n",
    "else:\n",
    "    unzip_to = train_data_dir\n",
    "\n",
    "\n",
    "def download_dataset(url):\n",
    "    if url.startswith(\"/content\"):\n",
    "        return url\n",
    "    elif \"drive.google.com\" in url:\n",
    "        os.chdir(root_dir)\n",
    "        !gdown --fuzzy {url}\n",
    "        return f\"{root_dir}/{zipfile_name}\"\n",
    "    elif \"huggingface.co\" in url:\n",
    "        if \"/blob/\" in url:\n",
    "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
    "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
    "        return f\"{root_dir}/{zipfile_name}\"\n",
    "    else:\n",
    "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
    "        return f\"{root_dir}/{zipfile_name}\"\n",
    "\n",
    "\n",
    "def extract_dataset(zip_file, output_path):\n",
    "    if zip_file.startswith(\"/content\"):\n",
    "        if \".tar\" in zip_file:\n",
    "          !tar -xvf {zip_file} -C \"{output_path}\"\n",
    "        else:\n",
    "          !unzip -j -o {zip_file} -d \"{output_path}\"\n",
    "    else:\n",
    "        if \".tar\" in zip_file:\n",
    "          !tar -xvf \"{zip_file}\" -C \"{output_path}\"\n",
    "        else:\n",
    "          !unzip -j -o \"{zip_file}\" -d \"{output_path}\"\n",
    "\n",
    "\n",
    "def remove_files(train_dir, files_to_move):\n",
    "    for filename in os.listdir(train_dir):\n",
    "        file_path = os.path.join(train_dir, filename)\n",
    "        if filename in files_to_move:\n",
    "            if not os.path.exists(file_path):\n",
    "                shutil.move(file_path, training_dir)\n",
    "            else:\n",
    "                os.remove(file_path)\n",
    "\n",
    "\n",
    "zip_file = download_dataset(zipfile_url)\n",
    "extract_dataset(zip_file, unzip_to)\n",
    "os.remove(zip_file)\n",
    "\n",
    "files_to_move = (\n",
    "    \"meta_cap.json\",\n",
    "    \"meta_cap_dd.json\",\n",
    "    \"meta_lat.json\",\n",
    "    \"meta_clean.json\",\n",
    ")\n",
    "\n",
    "remove_files(train_data_dir, files_to_move)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zlD24LEQ5Y3X"
   },
   "outputs": [],
   "source": [
    "# @title Create CSV from dataset. Assumes all images are the same file format.\n",
    "extension = \".png\" # @param {type:\"string\"}\n",
    "caption_extension = \".txt\" # @param {type:\"string\"}\n",
    "csv_output_path = \"/content/LoRA/dataset.csv\"\n",
    "import csv\n",
    "from pathlib import Path\n",
    "with open(csv_output_path, 'w', newline='') as csvfile:\n",
    "  csvwriter = csv.writer(csvfile)\n",
    "  csvwriter.writerow(['paths','caption'])\n",
    "  for filename in os.listdir(train_data_dir):\n",
    "    if filename.endswith(extension):\n",
    "      caption = \"\"\n",
    "      with open(os.path.join(train_data_dir,filename.replace(extension,caption_extension)), 'r') as captionfile:\n",
    "        caption = captionfile.read().rstrip()\n",
    "        #caption = caption.replace('\"','')\n",
    "        #caption = caption.replace(',','')\n",
    "        print(caption)\n",
    "      csvwriter.writerow([os.path.join(train_data_dir,filename),caption])\n",
    "\n",
    "with open(csv_output_path, \"r+\") as fn:\n",
    "  content = fn.read()\n",
    "  content = content.rstrip('\\n')\n",
    "  fn.seek(0)\n",
    "  fn.write(content)\n",
    "  fn.truncate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "d1PX_7m7bQnU"
   },
   "outputs": [],
   "source": [
    "# @title Set Parameters\n",
    "# @markdown The official example uses the same parameters for both prior and decoder, so I've replicated this here.<br>\n",
    "# @markdown Rank is hardcoded to 4. This is intentional, you WILL get NaN errors on higher ranks.\n",
    "output_to_drive = False # @param {type:'boolean'}\n",
    "if output_to_drive:\n",
    "  prior_output_dir='/content/drive/MyDrive/kandinsky_lora/prior'\n",
    "  decoder_output_dir='/content/drive/MyDrive/kandinsky_lora/decoder'\n",
    "else:\n",
    "  prior_output_dir='/content/kandinsky_lora/prior'\n",
    "  decoder_output_dir='/content/kandinsky_lora/decoder'\n",
    "mixed_precision = \"fp16\" # @param [\"fp16\",\"bf16\",\"none\"]\n",
    "image_resolution = 768 # @param {type:\"slider\", min:512, max:1024, step:128}\n",
    "train_batch_size = 1 # @param {type: \"integer\"}\n",
    "max_train_steps = 2500 # @param {type: \"integer\"}\n",
    "checkpointing_steps = 500 # @param {type: \"integer\"}\n",
    "gradient_accumulation_steps = 1 # @param {type: \"integer\"}\n",
    "lr = 1e-5 # @param {type: \"number\"}\n",
    "lr_scheduler = \"cosine\" # @param [\"constant\", \"constant_with_warmup\", \"cosine\" ]\n",
    "lr_warmup_steps = 0 # @param {type: \"integer\"}\n",
    "dataloader_num_workers = 0 # @param {type: \"integer\"}\n",
    "snr_gamma = 5.0 # @param {type: \"number\"}\n",
    "weight_decay = 0.0 # @param {type: \"number\"}\n",
    "train_seed = 0 # @param {type: \"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AfSvfop6UWqn"
   },
   "outputs": [],
   "source": [
    "# @title Train Decoder Lora\n",
    "command = (f'python3 /content/diffusers/examples/kandinsky2_2_train/tune_decoder_lora.py '\n",
    "           #f'--train_image_folder={train_data_dir} '\n",
    "           f'--train_images_paths_csv=/content/LoRA/dataset.csv '\n",
    "           f'--image_resolution={image_resolution} '\n",
    "           f'--train_batch_size={train_batch_size} '\n",
    "           f'--gradient_accumulation_steps={gradient_accumulation_steps} '\n",
    "           f'--gradient_checkpointing '\n",
    "           f'--mixed_precision={mixed_precision}  '\n",
    "           f'--max_train_steps={max_train_steps} '\n",
    "           f'--lr={lr} '\n",
    "           f'--max_grad_norm=1 '\n",
    "           f'--lr_scheduler={lr_scheduler} '\n",
    "           f'--lr_warmup_steps={lr_warmup_steps} '\n",
    "           f'--output_dir={decoder_output_dir} '\n",
    "           f'--rank=4 '\n",
    "           f'--snr_gamma={snr_gamma} '\n",
    "           f'--use_8bit_adam '\n",
    "           f'--checkpointing_steps={checkpointing_steps} '\n",
    "           f\"--dataloader_num_workers={dataloader_num_workers} \"\n",
    "           f\"--seed={train_seed} \"\n",
    "           f\"--weight_decay={weight_decay} \")\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "s2b8G0wxUblL"
   },
   "outputs": [],
   "source": [
    "# @title Train Prior Lora\n",
    "command = (f'python3 /content/diffusers/examples/kandinsky2_2_train/tune_prior_lora.py '\n",
    "           #f'--train_image_folder={train_data_dir} '\n",
    "           f'--train_images_paths_csv=/content/LoRA/dataset.csv '\n",
    "           f'--train_batch_size={train_batch_size} '\n",
    "           f'--gradient_accumulation_steps={gradient_accumulation_steps} '\n",
    "           f'--gradient_checkpointing '\n",
    "           f'--mixed_precision={mixed_precision}  '\n",
    "           f'--max_train_steps={max_train_steps} '\n",
    "           f'--lr={lr} '\n",
    "           f'--max_grad_norm=1 '\n",
    "           f'--lr_scheduler={lr_scheduler} '\n",
    "           f'--lr_warmup_steps={lr_warmup_steps} '\n",
    "           f'--output_dir={prior_output_dir} '\n",
    "           f'--rank={rank} '\n",
    "           f'--snr_gamma={snr_gamma} '\n",
    "           f'--use_8bit_adam '\n",
    "           f'--checkpointing_steps={checkpointing_steps} '\n",
    "           f\"--dataloader_num_workers={dataloader_num_workers} \"\n",
    "           f\"--seed={train_seed} \"\n",
    "           f\"--weight_decay={weight_decay} \")\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Convert To Safetensors\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "def shared_pointers(tensors):\n",
    "    ptrs = defaultdict(list)\n",
    "    for k, v in tensors.items():\n",
    "        ptrs[v.data_ptr()].append(k)\n",
    "    failing = []\n",
    "    for ptr, names in ptrs.items():\n",
    "        if len(names) > 1:\n",
    "            failing.append(names)\n",
    "    return failing\n",
    "\n",
    "# @markdown Input File\n",
    "pt_filename = \"\" # @param {type: \"string\"}\n",
    "# @markdown Output File\n",
    "sf_filename = \"\" # @param {type: \"string\"}\n",
    "loaded = torch.load(pt_filename, map_location=\"cpu\")\n",
    "if \"state_dict\" in loaded:\n",
    "    loaded = loaded[\"state_dict\"]\n",
    "shared = shared_pointers(loaded)\n",
    "for shared_weights in shared:\n",
    "    for name in shared_weights[1:]:\n",
    "        loaded.pop(name)\n",
    "\n",
    "# For tensors to be contiguous\n",
    "loaded = {k: v.contiguous() for k, v in loaded.items()}\n",
    "save_file(loaded, sf_filename, metadata={\"format\": \"pt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu57SPnxPhHg"
   },
   "source": [
    "## Inference\n",
    "This has very few optimizations applied, so it's only particularly useful for basic testing.\n",
    "Use Kubin for better inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "86O8LeqRpd4V"
   },
   "outputs": [],
   "source": [
    "# @title Inference Settings\n",
    "\n",
    "# @markdown ###LoRA settings\n",
    "use_decoder_lora = True # @param {type: \"boolean\"}\n",
    "use_prior_lora = True # @param {type: \"boolean\"}\n",
    "decoder_lora_path = \"\" # @param {type: \"string\"}\n",
    "prior_lora_path = \"\" # @param {type: \"string\"}\n",
    "\n",
    "# @markdown ###Generation settings\n",
    "prompt = \"\" # @param {type: \"string\"}\n",
    "negative_prompt = \"\" # @param {type: \"string\"}\n",
    "prior_steps = 25 # @param {type: \"integer\"}\n",
    "decoder_steps = 50 # @param {type: \"integer\"}\n",
    "width = 512 # @param {type: \"integer\"}\n",
    "height = 512 # @param {type: \"integer\"}\n",
    "seed = 1 # @param {type: \"integer\"}\n",
    "filename = \"demo.png\" # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lVaDPWIVKyNo"
   },
   "outputs": [],
   "source": [
    "# @title Load Pipelines\n",
    "import sys\n",
    "from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline\n",
    "import torch\n",
    "import PIL\n",
    "import torch\n",
    "from diffusers.utils import load_image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.models import UNet2DConditionModel\n",
    "import numpy as np\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained('kandinsky-community/kandinsky-2-2-prior', subfolder='image_encoder').to(torch.float16).to('cuda')\n",
    "unet = UNet2DConditionModel.from_pretrained('kandinsky-community/kandinsky-2-2-decoder', subfolder='unet').to(torch.float16).to('cuda')\n",
    "prior = KandinskyV22PriorPipeline.from_pretrained('kandinsky-community/kandinsky-2-2-prior', image_encoder=image_encoder, torch_dtype=torch.float16)\n",
    "prior = prior.to(\"cuda\")\n",
    "decoder = KandinskyV22Pipeline.from_pretrained('kandinsky-community/kandinsky-2-2-decoder', unet=unet, torch_dtype=torch.float16)\n",
    "decoder = decoder.to(\"cuda\")\n",
    "\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnAddedKVProcessor\n",
    "\n",
    "if use_decoder_lora:\n",
    "  lora_attn_procs = {}\n",
    "  d = torch.load(decoder_lora_path)\n",
    "  for name in decoder.unet.attn_processors.keys():\n",
    "      cross_attention_dim = None if name.endswith(\"attn1.processor\") else decoder.unet.config.cross_attention_dim\n",
    "      if name.startswith(\"mid_block\"):\n",
    "          hidden_size = decoder.unet.config.block_out_channels[-1]\n",
    "      elif name.startswith(\"up_blocks\"):\n",
    "          block_id = int(name[len(\"up_blocks.\")])\n",
    "          hidden_size = list(reversed(decoder.unet.config.block_out_channels))[block_id]\n",
    "      elif name.startswith(\"down_blocks\"):\n",
    "          block_id = int(name[len(\"down_blocks.\")])\n",
    "          hidden_size = decoder.unet.config.block_out_channels[block_id]\n",
    "      lora_attn_procs[name] = LoRAAttnAddedKVProcessor(\n",
    "              hidden_size=hidden_size,\n",
    "              cross_attention_dim=cross_attention_dim,\n",
    "              rank=rank,\n",
    "      ).to('cuda')\n",
    "  decoder.unet.set_attn_processor(lora_attn_procs)\n",
    "  decoder.unet.load_state_dict(d, strict=False)\n",
    "\n",
    "if use_prior_lora:\n",
    "  lora_attn_procs = {}\n",
    "  for name in prior.prior.attn_processors.keys():\n",
    "      lora_attn_procs[name] = LoRAAttnProcessor(hidden_size=2048).to('cuda')\n",
    "  prior.prior.set_attn_processor(lora_attn_procs)\n",
    "  prior.prior.load_state_dict(torch.load(prior_lora_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "YDUexFP0K-y5"
   },
   "outputs": [],
   "source": [
    "# @title Generate\n",
    "torch.manual_seed(seed)\n",
    "img_emb = prior(prompt=prompt, num_inference_steps=prior_steps, num_images_per_prompt=1)\n",
    "neg_emb = prior(prompt=negative_prompt, num_inference_steps=prior_steps, num_images_per_prompt=1)\n",
    "images = decoder(image_embeds=img_emb.image_embeds, negative_image_embeds=neg_emb.image_embeds, num_inference_steps=decoder_steps, height=height, width=width)\n",
    "images.images[0].save(filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
